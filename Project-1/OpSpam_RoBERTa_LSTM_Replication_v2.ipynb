{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# OpSpam Replication: RoBERTa(+LSTM) **and** TFâ€‘IDF + Logistic Baseline\n\nThis notebook is pre-configured for your CSV with columns:\n- **`text`**: review text\n- **`deceptive`**: binary label (1 = deceptive, 0 = truthful)\n\nIt trains **two models** and reports comparable metrics/plots:\n1. **Baseline:** TFâ€‘IDF + Logistic Regression (mirrors your demo)\n2. **RoBERTa (+ optional LSTM)** classifier\n\nExports: metrics CSVs, confusion matrix, ROC and PR curves, and a sideâ€‘byâ€‘side table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title â¬‡ï¸ Install dependencies\n!pip -q install transformers==4.44.2 datasets==2.21.0 accelerate==0.34.2 scikit-learn==1.5.1 matplotlib==3.9.0 torch==2.3.1 -U\n\nimport os, random, math, json, sys, time, re\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, average_precision_score,\n    classification_report, roc_curve, precision_recall_curve\n)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nfrom transformers import (\n    AutoTokenizer, AutoModel, AutoConfig,\n    TrainingArguments, Trainer\n)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title âš™ï¸ Configuration\nMODEL_NAME = \"roberta-base\"  #@param [\"roberta-base\"]\nMAX_LEN = 256  #@param {type:\"integer\"}\nBATCH_SIZE = 32  #@param {type:\"integer\"}\nEPOCHS = 5  #@param {type:\"integer\"}\nLEARNING_RATE = 2e-5  #@param {type:\"number\"}\nWEIGHT_DECAY = 0.01  #@param {type:\"number\"}\nSEED = 42  #@param {type:\"integer\"}\nUSE_LSTM = True  #@param {type:\"boolean\"}\nLSTM_HIDDEN = 128  #@param {type:\"integer\"}\nDROPOUT = 0.6  #@param {type:\"number\"}\n\n# Baseline config\nTFIDF_MAX_FEATURES = 5000  #@param {type:\"integer\"}\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ“¥ Load data\ncsv_path = \"\"  #@param {type:\"string\"}\n\nif not csv_path or not os.path.exists(csv_path):\n    try:\n        from google.colab import files  # type: ignore\n        print(\"Upload 'deceptive-opinion.csv'\")\n        uploaded = files.upload()\n        csv_path = list(uploaded.keys())[0]\n    except Exception as e:\n        print(\"No Colab file picker. Falling back to default name.\")\n        csv_path = \"deceptive-opinion.csv\"\n\nassert os.path.exists(csv_path), f\"CSV not found at {csv_path}\"\ndf = pd.read_csv(csv_path)\nprint(\"Columns:\", df.columns.tolist())\ndisplay(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Columns & Quick Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ”Ž Normalize expected columns\nTEXT_COL = \"text\"      # fixed per your demo\nLABEL_COL = \"deceptive\"  # fixed per your demo\n\nassert TEXT_COL in df.columns, f\"'{TEXT_COL}' not found\"\nassert LABEL_COL in df.columns, f\"'{LABEL_COL}' not found\"\n\n# Ensure labels are 0/1 integers\nif df[LABEL_COL].dtype != int and df[LABEL_COL].dtype != np.int64:\n    df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors=\"coerce\").astype(\"Int64\").fillna(0).astype(int)\n\n# Quick text clean like your demo\ndf[TEXT_COL] = df[TEXT_COL].astype(str).str.lower().str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n\nprint(\"Label distribution:\", df[LABEL_COL].value_counts().to_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ”€ Stratified split (80/20) and make a val split from train\nX = df[TEXT_COL].values\ny = df[LABEL_COL].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=SEED, stratify=y\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.10, random_state=SEED, stratify=y_train\n)\n\nprint(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline (TFâ€‘IDF + Logistic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ§ª Baseline: TFâ€‘IDF + Logistic Regression\ntfidf = TfidfVectorizer(stop_words='english', max_features=TFIDF_MAX_FEATURES)\nX_train_t = tfidf.fit_transform(X_train)\nX_val_t   = tfidf.transform(X_val)\nX_test_t  = tfidf.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=200)\nlogreg.fit(X_train_t, y_train)\n\ny_prob_lr = logreg.predict_proba(X_test_t)[:,1]\ny_pred_lr = (y_prob_lr >= 0.5).astype(int)\n\nacc_lr  = accuracy_score(y_test, y_pred_lr)\nprec_lr = precision_score(y_test, y_pred_lr, zero_division=0)\nrec_lr  = recall_score(y_test, y_pred_lr, zero_division=0)\nf1_lr   = f1_score(y_test, y_pred_lr, zero_division=0)\nauroc_lr = roc_auc_score(y_test, y_prob_lr)\nap_lr    = average_precision_score(y_test, y_prob_lr)\n\nprint(\"=== TFâ€‘IDF + Logistic (Test) ===\")\nprint(f\"Accuracy : {acc_lr:.4f}\")\nprint(f\"Precision: {prec_lr:.4f}\")\nprint(f\"Recall   : {rec_lr:.4f}\")\nprint(f\"F1       : {f1_lr:.4f}\")\nprint(f\"AUROC    : {auroc_lr:.4f}\")\nprint(f\"AP       : {ap_lr:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_lr, digits=4))\n\n# Plots\nfpr, tpr, _ = roc_curve(y_test, y_prob_lr)\nplt.figure()\nplt.plot(fpr, tpr, label=f\"AUROC = {auroc_lr:.3f}\")\nplt.plot([0,1],[0,1], linestyle=\"--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve â€” TFâ€‘IDF + Logistic\")\nplt.legend(loc=\"lower right\"); plt.grid(True, linestyle=\":\"); plt.tight_layout()\nplt.savefig(\"roc_curve_logreg.png\", dpi=160); plt.show()\n\nprecisions, recalls, _ = precision_recall_curve(y_test, y_prob_lr)\nplt.figure()\nplt.plot(recalls, precisions, label=f\"AP = {ap_lr:.3f}\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\nplt.title(\"PR Curve â€” TFâ€‘IDF + Logistic\")\nplt.legend(loc=\"lower left\"); plt.grid(True, linestyle=\":\"); plt.tight_layout()\nplt.savefig(\"pr_curve_logreg.png\", dpi=160); plt.show()\n\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nplt.figure()\nplt.imshow(cm_lr, interpolation='nearest')\nplt.title(\"Confusion Matrix â€” TFâ€‘IDF + Logistic\")\nplt.colorbar()\nticks = np.arange(2)\nplt.xticks(ticks, [\"Truthful (0)\",\"Deceptive (1)\"])\nplt.yticks(ticks, [\"Truthful (0)\",\"Deceptive (1)\"])\nfor i in range(cm_lr.shape[0]):\n    for j in range(cm_lr.shape[1]):\n        plt.text(j, i, format(cm_lr[i, j], 'd'), ha=\"center\", va=\"center\")\nplt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout()\nplt.savefig(\"cm_logreg.png\", dpi=160); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer & Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ”¡ Tokenize for RoBERTa\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts; self.labels = labels\n        self.tokenizer = tokenizer; self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            str(self.texts[idx]), padding=\"max_length\", truncation=True,\n            max_length=self.max_len, return_tensors=\"pt\"\n        )\n        item = {k: v.squeeze(0) for k, v in enc.items()}\n        item[\"labels\"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n        return item\n\ntrain_ds = TextDataset(X_train, y_train, tokenizer, MAX_LEN)\nval_ds   = TextDataset(X_val, y_val, tokenizer, MAX_LEN)\ntest_ds  = TextDataset(X_test, y_test, tokenizer, MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ§  RoBERTa(+LSTM) model\nclass RobertaLSTMClassifier(nn.Module):\n    def __init__(self, model_name, lstm_hidden=128, dropout=0.6, use_lstm=True):\n        super().__init__()\n        self.use_lstm = use_lstm\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.roberta = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n        hidden = self.config.hidden_size\n        if self.use_lstm:\n            self.lstm = nn.LSTM(input_size=hidden, hidden_size=lstm_hidden,\n                                num_layers=1, batch_first=True, bidirectional=False)\n            feat = lstm_hidden\n        else:\n            feat = hidden\n        self.bn = nn.BatchNorm1d(feat)\n        self.drop = nn.Dropout(dropout)\n        self.cls = nn.Linear(feat, 2)\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        out = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        last = out.last_hidden_state\n        if self.use_lstm:\n            _, (h, _) = self.lstm(last)\n            feat = h.squeeze(0)\n        else:\n            feat = last[:,0,:]\n        feat = self.bn(feat)\n        feat = self.drop(feat)\n        logits = self.cls(feat)\n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train (RoBERTa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸš€ Train RoBERTa(+LSTM)\nfrom transformers import TrainingArguments, Trainer\n\nmodel = RobertaLSTMClassifier(MODEL_NAME, LSTM_HIDDEN, DROPOUT, USE_LSTM)\n\ndef compute_metrics_fn(p):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    probs = torch.softmax(torch.tensor(preds), dim=-1).numpy()[:,1]\n    y_pred = (probs >= 0.5).astype(int)\n    y_true = p.label_ids\n    return {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n        \"auroc\": roc_auc_score(y_true, probs),\n        \"ap\": average_precision_score(y_true, probs),\n    }\n\nargs = TrainingArguments(\n    output_dir=\"outputs_roberta\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_steps=50,\n    report_to=[], seed=SEED\n)\n\ntrainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=compute_metrics_fn)\ntrainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate (RoBERTa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ“Š Evaluate RoBERTa(+LSTM) & plot\npred = trainer.predict(test_ds)\nprobs_rb = torch.softmax(torch.tensor(pred.predictions), dim=-1).numpy()[:,1]\ny_true = pred.label_ids\ny_pred_rb = (probs_rb >= 0.5).astype(int)\n\nacc_rb  = accuracy_score(y_true, y_pred_rb)\nprec_rb = precision_score(y_true, y_pred_rb, zero_division=0)\nrec_rb  = recall_score(y_true, y_pred_rb, zero_division=0)\nf1_rb   = f1_score(y_true, y_pred_rb, zero_division=0)\nauroc_rb = roc_auc_score(y_true, probs_rb)\nap_rb    = average_precision_score(y_true, probs_rb)\n\nprint(\"=== RoBERTa(+LSTM) (Test) ===\")\nprint(f\"Accuracy : {acc_rb:.4f}\")\nprint(f\"Precision: {prec_rb:.4f}\")\nprint(f\"Recall   : {rec_rb:.4f}\")\nprint(f\"F1       : {f1_rb:.4f}\")\nprint(f\"AUROC    : {auroc_rb:.4f}\")\nprint(f\"AP       : {ap_rb:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_true, y_pred_rb, digits=4))\n\n# ROC\nfpr_rb, tpr_rb, _ = roc_curve(y_true, probs_rb)\nplt.figure()\nplt.plot(fpr_rb, tpr_rb, label=f\"RoBERTa AUROC={auroc_rb:.3f}\")\nplt.plot([0,1],[0,1], linestyle=\"--\")\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve â€” RoBERTa(+LSTM)\")\nplt.legend(loc=\"lower right\"); plt.grid(True, linestyle=\":\"); plt.tight_layout()\nplt.savefig(\"roc_curve_roberta.png\", dpi=160); plt.show()\n\n# PR\nprec_rb_c, rec_rb_c, _ = precision_recall_curve(y_true, probs_rb)\nplt.figure()\nplt.plot(rec_rb_c, prec_rb_c, label=f\"RoBERTa AP={ap_rb:.3f}\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\nplt.title(\"PR Curve â€” RoBERTa(+LSTM)\")\nplt.legend(loc=\"lower left\"); plt.grid(True, linestyle=\":\"); plt.tight_layout()\nplt.savefig(\"pr_curve_roberta.png\", dpi=160); plt.show()\n\n# Confusion matrix\ncm_rb = confusion_matrix(y_true, y_pred_rb)\nplt.figure()\nplt.imshow(cm_rb, interpolation='nearest')\nplt.title(\"Confusion Matrix â€” RoBERTa(+LSTM)\")\nplt.colorbar()\nticks = np.arange(2)\nplt.xticks(ticks, [\"Truthful (0)\",\"Deceptive (1)\"])\nplt.yticks(ticks, [\"Truthful (0)\",\"Deceptive (1)\"])\nfor i in range(cm_rb.shape[0]):\n    for j in range(cm_rb.shape[1]):\n        plt.text(j, i, format(cm_rb[i, j], 'd'), ha=\"center\", va=\"center\")\nplt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout()\nplt.savefig(\"cm_roberta.png\", dpi=160); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n#@title ðŸ§¾ Side-by-side table\nside = pd.DataFrame({\n    \"Metric\": [\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"AUROC\",\"AP\"],\n    \"TF-IDF+LogReg\": [acc_lr, prec_lr, rec_lr, f1_lr, auroc_lr, ap_lr],\n    \"RoBERTa(+LSTM)\": [acc_rb, prec_rb, rec_rb, f1_rb, auroc_rb, ap_rb],\n})\ndisplay(side)\nside.to_csv(\"baseline_vs_roberta.csv\", index=False)\nprint(\"Saved: baseline_vs_roberta.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}